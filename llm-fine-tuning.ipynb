{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42eda1b3",
   "metadata": {},
   "source": [
    "Below is a complete, step‐by‐step guide for fine‑tuning (a.k.a. retraining) a pre‑trained language model using the Hugging Face Transformers library. In this guide, you’ll learn how to set up VS Code with GitHub integration, prepare your Python virtual environment with the needed libraries, load and preprocess a dataset using Hugging Face’s [datasets](https://huggingface.co/docs/datasets/) library, fine‑tune a model (we’ll use GPT‑2 as an example), and then test your retrained model with a simple text‑generation case.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setting Up Your Environment in VS Code and GitHub\n",
    "\n",
    "### **a. VS Code & GitHub Initialization**\n",
    "\n",
    "1. **Create a Project Folder:**\n",
    "   Create a new folder on your computer (e.g., `llm-fine-tuning`) that will house your project files.\n",
    "\n",
    "2. **Open in VS Code:**\n",
    "   Launch VS Code and open your new folder.\n",
    "   - **Tip:** Install the official [Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python) for enhanced language support.\n",
    "   - **Also:** Use the built‑in Git extension. In the VS Code sidebar, click the Source Control icon and initialize a Git repository for your folder. Then push it to GitHub (create a new repository on GitHub and follow the onscreen instructions).\n",
    "\n",
    "3. **Create a Virtual Environment:**\n",
    "   Open your integrated terminal in VS Code (``Ctrl+` `` on Windows/Linux or `Cmd+`` on macOS) and run:\n",
    "   ```bash\n",
    "   python -m venv venv\n",
    "   ```\n",
    "   Then activate it:\n",
    "   - **Windows:**\n",
    "     ```bash\n",
    "     venv\\Scripts\\activate\n",
    "     ```\n",
    "   - **macOS/Linux:**\n",
    "     ```bash\n",
    "     source venv/bin/activate\n",
    "     ```\n",
    "\n",
    "4. **Create a `requirements.txt` File:**\n",
    "   In your project root, create a file named `requirements.txt` and add:\n",
    "   ```\n",
    "   transformers\n",
    "   datasets\n",
    "   torch\n",
    "   accelerate\n",
    "   ```\n",
    "   Save the file and install the dependencies with:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "   ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444739ed",
   "metadata": {},
   "source": [
    "## 2. Selecting a Dataset and Defining a Retraining Goal\n",
    "\n",
    "### **a. Choosing a Dataset**\n",
    "\n",
    "For demonstration, we’ll use the **Wikitext-2** dataset—a popular benchmark dataset for language modeling tasks. It is publicly available via the Hugging Face datasets library.\n",
    "\n",
    "Other dataset options include:\n",
    "- **BookCorpus/OpenWebText:** Great for more narrative training.\n",
    "- **Your Custom Dataset:** If you have domain‑specific text data, you can format it (e.g., in CSV or JSON) and load it with Hugging Face’s [`load_dataset`](https://huggingface.co/docs/datasets/load_dataset) function.\n",
    "- **Pandas or CSV Libraries:** For custom local data, you might use `pandas` to load and then convert to the Hugging Face Dataset format.\n",
    "\n",
    "### **b. Retraining Goal**\n",
    "\n",
    "For this tutorial, our goal is to **fine‑tune GPT‑2** so that after training, it better models the kind of text found in Wikitext-2. Once fine‑tuned, you can provide prompts and evaluate if the model outputs text that both follows the learned style and maintains coherence. Later, you could extend this approach to domain‑specific data for tasks like summarization or dialogue generation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7d38e",
   "metadata": {},
   "source": [
    "## 3. The Code: Fine‑Tuning a Pre‑Trained Model with Hugging Face Transformers\n",
    "\n",
    "Create a new file (for example, `main.py`) in your project and populate it with the following code. This script will:\n",
    "\n",
    "1. Load the pre‑trained GPT‑2 model and its tokenizer.\n",
    "2. Load and preprocess the Wikitext‑2 dataset.\n",
    "3. Use the Hugging Face `Trainer` API to fine‑tune the model.\n",
    "4. Save the fine‑tuned model for later use.\n",
    "\n",
    "### **`main.py`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fdb70",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/workspaces/llm-fine-tuning/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "def main():\n",
    "    # 1. Load the Pre‑Trained Model and Tokenizer\n",
    "    model_name = \"gpt2\"  # You can experiment with other models from Hugging Face Model Hub.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # 2. Load and Pre‑process the Dataset\n",
    "    # We use the 'wikitext' dataset, particularly the 'wikitext-2-raw-v1' variant.\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "    # Define a tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    # Tokenize the dataset in batches for speed\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Use the 'train' and 'validation' splits for training and evaluation respectively.\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "    # 3. Set Up Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=2,   # Adjust batch sizes based on your GPU RAM\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=1,              # Increase epochs for better fine‑tuning\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,              # Keep only the 2 most recent checkpoints\n",
    "        logging_steps=100,               # Log training info every 100 steps\n",
    "        push_to_hub=False              # You can set this to True if you plan to push to Hugging Face Hub\n",
    "    )\n",
    "\n",
    "    # 4. Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    # 5. Fine‑Tune the Model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 6. Save the Fine‑Tuned Model and Tokenizer\n",
    "    model_save_path = \"./fine_tuned_model\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
